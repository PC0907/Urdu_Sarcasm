{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/PC0907/Urdu_Sarcasm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:35:44.173492Z","iopub.execute_input":"2025-02-03T08:35:44.173768Z","iopub.status.idle":"2025-02-03T08:35:45.923488Z","shell.execute_reply.started":"2025-02-03T08:35:44.173739Z","shell.execute_reply":"2025-02-03T08:35:45.922457Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Urdu_Sarcasm'...\nremote: Enumerating objects: 19, done.\u001b[K\nremote: Counting objects: 100% (19/19), done.\u001b[K\nremote: Compressing objects: 100% (15/15), done.\u001b[K\nremote: Total 19 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (19/19), 2.79 MiB | 9.42 MiB/s, done.\nResolving deltas: 100% (2/2), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.getcwd()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:35:45.924481Z","iopub.execute_input":"2025-02-03T08:35:45.924764Z","iopub.status.idle":"2025-02-03T08:35:45.932003Z","shell.execute_reply.started":"2025-02-03T08:35:45.924736Z","shell.execute_reply":"2025-02-03T08:35:45.931177Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"cd Urdu_Sarcasm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:35:45.932862Z","iopub.execute_input":"2025-02-03T08:35:45.933090Z","iopub.status.idle":"2025-02-03T08:35:45.958139Z","shell.execute_reply.started":"2025-02-03T08:35:45.933070Z","shell.execute_reply":"2025-02-03T08:35:45.957178Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Urdu_Sarcasm\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"cd Data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:35:45.959126Z","iopub.execute_input":"2025-02-03T08:35:45.959428Z","iopub.status.idle":"2025-02-03T08:35:45.974032Z","shell.execute_reply.started":"2025-02-03T08:35:45.959401Z","shell.execute_reply":"2025-02-03T08:35:45.973217Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/Urdu_Sarcasm/Data\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Model Architecture","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertTokenizer, BertModel, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:35:45.974902Z","iopub.execute_input":"2025-02-03T08:35:45.975102Z","iopub.status.idle":"2025-02-03T08:36:05.881010Z","shell.execute_reply.started":"2025-02-03T08:35:45.975084Z","shell.execute_reply":"2025-02-03T08:36:05.880106Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import BertTokenizer, BertModel\n\nclass SarcasmDetector(nn.Module):\n    def __init__(self, bert_model, hidden_size=256, num_attention_heads=8):\n        super(SarcasmDetector, self).__init__()\n        self.bert = bert_model\n        self.bert_hidden_size = 768\n        self.hidden_size = hidden_size\n        \n        # BiGRU layer\n        self.bigru = nn.GRU(\n            input_size=self.bert_hidden_size,\n            hidden_size=hidden_size,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        # Multi-head attention layer\n        self.multihead_attn = nn.MultiheadAttention(\n            embed_dim=hidden_size * 2,  # BiGRU output dimension (hidden_size * 2 for bidirectional)\n            num_heads=num_attention_heads,\n            batch_first=True\n        )\n        \n        # Layer normalization\n        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n        \n        # Output layers\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Sequential(\n            nn.Linear(hidden_size * 2, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(hidden_size, 2)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Get BERT outputs\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_sequence = bert_outputs.last_hidden_state\n        \n        # Pass through BiGRU\n        gru_output, _ = self.bigru(bert_sequence)\n        \n        # Apply multi-head attention\n        # Using self-attention: query, key, and value are all the same\n        attn_output, _ = self.multihead_attn(gru_output, gru_output, gru_output)\n        \n        # Add residual connection and layer normalization\n        attn_output = self.layer_norm(attn_output + gru_output)\n        \n        # Pool the attention output (using mean pooling)\n        pooled_output = torch.mean(attn_output, dim=1)\n        \n        # Pass through classifier\n        output = self.classifier(pooled_output)\n        return output\n\nclass UrduSarcasmDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        # Get BERT encodings\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:36:05.883383Z","iopub.execute_input":"2025-02-03T08:36:05.883868Z","iopub.status.idle":"2025-02-03T08:36:05.892663Z","shell.execute_reply.started":"2025-02-03T08:36:05.883845Z","shell.execute_reply":"2025-02-03T08:36:05.891858Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def prepare_data(texts, labels, tokenizer, max_length=128, test_size=0.2, random_state=42):\n    \"\"\"\n    Prepare train and test datasets with an 80:20 split\n    \"\"\"\n    # Perform train-test split\n    train_texts, test_texts, train_labels, test_labels = train_test_split(\n        texts, labels, test_size=test_size, random_state=random_state\n    )\n\n    # Create datasets\n    train_dataset = UrduSarcasmDataset(train_texts, train_labels, tokenizer, max_length)\n    test_dataset = UrduSarcasmDataset(test_texts, test_labels, tokenizer, max_length)\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n    return train_loader, test_loader\n\n\ndef calculate_metrics(true_labels, predictions):\n    \"\"\"\n    Calculate all metrics in one place to avoid code duplication\n    \"\"\"\n    accuracy = accuracy_score(true_labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        true_labels, predictions, average='binary', zero_division=0\n    )\n\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:36:05.893791Z","iopub.execute_input":"2025-02-03T08:36:05.894082Z","iopub.status.idle":"2025-02-03T08:36:05.935236Z","shell.execute_reply.started":"2025-02-03T08:36:05.894055Z","shell.execute_reply":"2025-02-03T08:36:05.934537Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_model(model, train_loader, device, num_epochs=20, patience=3):\n    optimizer = AdamW(model.parameters(), lr=2e-5)\n    criterion = nn.CrossEntropyLoss()\n    best_loss = float('inf')\n    patience_counter = 0\n    best_model = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        predictions = []\n        true_labels = []\n\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)  # Removed emoji_ids\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            predictions.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n        avg_loss = total_loss / len(train_loader)\n        metrics = calculate_metrics(true_labels, predictions)\n\n        print(f'\\nEpoch {epoch + 1}/{num_epochs}:')\n        print(f'Training - Loss: {avg_loss:.4f}, Accuracy: {metrics[\"accuracy\"]:.4f}, '\n              f'F1: {metrics[\"f1\"]:.4f}, Precision: {metrics[\"precision\"]:.4f}, '\n              f'Recall: {metrics[\"recall\"]:.4f}')\n\n        if avg_loss < best_loss:\n            best_loss = avg_loss\n            patience_counter = 0\n            best_model = model.state_dict().copy()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f'\\nEarly stopping triggered after epoch {epoch + 1}')\n                model.load_state_dict(best_model)\n                break\n\n    return model\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc='Evaluating'):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            outputs = model(input_ids, attention_mask)  # Removed emoji_ids\n            _, predicted = torch.max(outputs, 1)\n\n            predictions.extend(predicted.cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n\n    metrics = calculate_metrics(true_labels, predictions)\n    metrics['detailed_report'] = classification_report(true_labels, predictions)\n\n    return metrics\n\ndef calculate_metrics(true_labels, predictions):\n    \"\"\"Helper function to calculate various metrics\"\"\"\n    return {\n        'accuracy': accuracy_score(true_labels, predictions),\n        'precision': precision_score(true_labels, predictions, average='binary'),\n        'recall': recall_score(true_labels, predictions, average='binary'),\n        'f1': f1_score(true_labels, predictions, average='binary')\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:36:05.936097Z","iopub.execute_input":"2025-02-03T08:36:05.936377Z","iopub.status.idle":"2025-02-03T08:36:05.955005Z","shell.execute_reply.started":"2025-02-03T08:36:05.936355Z","shell.execute_reply":"2025-02-03T08:36:05.954360Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def main():\n    # Load Preprocessed Data\n    df = pd.read_csv('preprocessed_data.csv')\n\n    # Initialize Tokenizer and BERT Model\n    tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n    bert_model = BertModel.from_pretrained('bert-base-multilingual-cased')\n\n    # Prepare data using our prepare_data function\n    train_loader, test_loader = prepare_data(\n        texts=df['Preprocessed'].values,\n        labels=df['is_sarcastic'].values,\n        tokenizer=tokenizer,\n        max_length=128,\n        test_size=0.2,\n        random_state=42\n    )\n\n    # Initialize Model and Device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = SarcasmDetector(bert_model).to(device)\n\n    # Train the Model\n    print(\"Training the model...\")\n    trained_model = train_model(\n        model=model,\n        train_loader=train_loader,\n        device=device,\n        num_epochs=30,\n        patience=3\n    )\n\n    # Evaluate on test set\n    print(\"\\nEvaluating on test set...\")\n    test_metrics = evaluate_model(trained_model, test_loader, device)\n\n    # Print results\n    print(\"\\nTest Set Metrics:\")\n    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n    print(f\"Precision: {test_metrics['precision']:.4f}\")\n    print(f\"Recall: {test_metrics['recall']:.4f}\")\n    print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n    print(\"\\nDetailed Classification Report:\")\n    print(test_metrics['detailed_report'])\n\n    # Save the model\n    print(\"\\nSaving the model...\")\n    torch.save(trained_model.state_dict(), 'sarcasm_detector.pth')\n\n    # Save the metrics\n    print(\"Saving the metrics...\")\n    with open('test_metrics.txt', 'w') as f:\n        f.write(\"Test Set Metrics:\\n\")\n        f.write(f\"Accuracy: {test_metrics['accuracy']:.4f}\\n\")\n        f.write(f\"Precision: {test_metrics['precision']:.4f}\\n\")\n        f.write(f\"Recall: {test_metrics['recall']:.4f}\\n\")\n        f.write(f\"F1 Score: {test_metrics['f1']:.4f}\\n\")\n        f.write(\"\\nDetailed Classification Report:\\n\")\n        f.write(test_metrics['detailed_report'])\n\n    print(\"Training and evaluation completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:36:05.955908Z","iopub.execute_input":"2025-02-03T08:36:05.956181Z","iopub.status.idle":"2025-02-03T08:36:05.974121Z","shell.execute_reply.started":"2025-02-03T08:36:05.956148Z","shell.execute_reply":"2025-02-03T08:36:05.973377Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-03T11:01:31.557Z"}},"outputs":[],"execution_count":null}]}